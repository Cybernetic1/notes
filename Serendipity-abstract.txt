In recently years, as we are at the height of the information age, the amount of information that needs processing has grown exponentially, and text classification becomes a hot topic.  The text classification task is mainly divided into corpus pre-processing and model-building.  During pre-processing, parts-of-speech tagging (POS-tagging) is of critical importance.  POS-tagging with high accuracy and efficiency is urgently needed in current machine learning applications.

Our research is carried out in 2 stages, namely: How does domain-specificity influence POS-tagging?  What is the effect of biased or unlabelled training corpus on POS-tagging?  In the first stage, we want to tackle the problem of bad domain-adaptability due to limited lexicons, which we resolve by building domain-specific lexicons, using a dynamic programming algorithm to increase domain-adaptability.  In experiment, we apply our technique to construct the Jieba ("jabber"), Smallseg, and Snailseg models.  Our result shows that, without domain-specific lexicon, Jieba has higher accuracy and recall rate than Smallseg and Snailseg.  In lexicon-enhanced Jieba, the recall rate increased roughly 1% and accuracy increased about 8%.

Needless to say, our current research is just a beginning in the field of text classification.  In the future, we plan to consider more techniques such as multiple sampling, and combining or aggregating models with weights.

近年来，随着数据时代的到来，信息规模呈现几何式增长，文本分类成为热门研究之一。虽然已经有很多针对分类性能改进的方法，但随着信息库的快速膨胀，执行分类算法时，面临如何进行文本分词、数据分布、标签和分类算法等问题。
本课题首先通过构建领域词典，采用基于领域词典的动态规划分词算法来提高动态规划分词算法的领域适应性，解决了文本分词领域受限对文本分类结果影响的问题。在实验部分，用构建的分词方法与Smallseg分词、Snailseg分词来对特定领域的分词结构进行分词，实验结果表明未添加领域词典的结巴分词的准确率和召回率都高于Smallseg分词、Snailseg分词。添加领域词典的结巴分词比未添加领域词典之前，分词召回率提升了大约1%，分词准确率提升了大约8%。然后基于构建的分词算法，进一步提出了融合K-means、卷积神经网络（Convolutional Neural Networks，简称CNN）、双向长短时记忆网络（Bi-directional Recurrent Neural Network，简称Bi-LSTM） 的模型（简称KCBL）。 该模型使用K-means对情感文本聚类后重采样以调整数据分布，然后在K-means和CNN-Bi-LSTM之间构造损失函数。 与其它CNN-Bi-LSTM模型不同，KCBL可以针对不同的语料调整数据分布，并为不同的数据分布构建端到端学习，解决了文本数据分布不平衡和打标签难的问题。为了验证KCBL的性能，本课题将其应用在8个语料库上与7个主流文本分类器（CNN, LSTM, Bi-LSTM, CapsNet, CNN-LSTM, CNN-Bi-LSTM, K-means 融合 CNN-LSTM(简称KCL)）  在四个不同的评价指标 （准确性、召回率、F1评分和AUC） 下进行比较，可以得到在8个文本数据集上与对比的7个分类器而言KCBL表现优异。 此外，显著性测试结果表明KCBL与其他7个主流文本分类器相比，可以实现极高的竞争性能。综上所述，KCBL效果优于主流的文本分类算法，可应用于文本分类。

In recently years, as we are at the height of the information age, the amount of information that needs processing has grown exponentially, and text classification is now a hot research topic.  Though there exist various ways to improve the quality of text classification, as the volume of text corpuses grow, we are faced with the sub-tasks of word segmentation, document class distribution, labelling, and word classification.  Our research starts from building domain-specific lexicons, using a dynamic programming word classification algorithm to increase domain adaptability.  This resolves the problem of domain-specific words causing bad classification.  In experiment, we constructed the Jieba ('jabber') word classification model to compare with the Smallseg and Snailseg models.  Experimental results shows that, without domain-specific lexicon, Jieba has higher accuracy and recall rate than Smallseg and Snailseg.  In lexicon-enhanced Jieba, the recall rate increased roughly 1% and accuracy increased about 8%.
Further, based on the model we constructed, we combined K-means, CNN (convolutional neural networks) and Bi-LSTM (bi-directional long-short term memory) techniques to build a new model  (abbreviated KCBL).  In this model, we applied K-means clustering on sentiments and re-sampled documents to normalize their distribution.  For each document cluster we constructed end-to-end learners, thus solving the problem of uneven distribution of text data and difficulties in labelling them.  To verify the performance of KCBL, we applied it on 8 corpora and compared it againt 7 mainstream classifiers: CNN, LSTM, Bi-LSTM, CapsNet, CNN-LSTM, CNN-Bi-LSTM, K-means + CNN-LSTM (= KCL in short).  By 4 different assessment benchmarks (accuracy, recall rate, F1 score, and AUC), we found superior performance of KCBL against 7 classifiers on 8 corpuses.  Moreover, statistical significance shows that KCBL is highly competitive compared with the other 7 classifiers.  In summary, we conclude that KCBL out-performs mainstream classifiers and is very suitable for text classification.
